import requests
import logging
import time
import re
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Set
from bs4 import BeautifulSoup
from app.config import Config
from app.models import Exploit
from app.models.base import get_db

class ExploitDBScraper:
    """Scraper for the Exploit Database (exploit-db.com)."""
    
    def __init__(self):
        self.base_url = "https://www.exploit-db.com"
        self.search_url = f"{self.base_url}/search"
        self.api_url = f"{self.base_url}/api/v1/search"
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'CVEhive/1.0 (Security Research Tool)',
            'Accept': 'application/json, text/html, */*',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # Platform mappings
        self.platform_keywords = {
            'windows': ['windows', 'win32', 'win64', 'microsoft'],
            'linux': ['linux', 'unix', 'ubuntu', 'debian', 'centos', 'redhat'],
            'web': ['php', 'asp', 'jsp', 'web', 'http', 'html'],
            'hardware': ['hardware', 'firmware', 'embedded', 'iot'],
            'multiple': ['multiple', 'cross-platform', 'universal']
        }
    
    def search_exploits_for_cve(self, cve_id: str, limit: int = 50) -> List[Dict]:
        """
        Search for exploits related to a specific CVE.
        
        Args:
            cve_id (str): CVE identifier (e.g., CVE-2023-1234)
            limit (int): Maximum number of results to return
            
        Returns:
            List[Dict]: List of exploit data
        """
        exploits = []
        
        try:
            # Try API search first
            api_results = self._search_via_api(cve_id, limit)
            if api_results:
                exploits.extend(api_results)
            
            # If API doesn't return enough results, try web scraping
            if len(exploits) < limit:
                web_results = self._search_via_web(cve_id, limit - len(exploits))
                exploits.extend(web_results)
            
            # Remove duplicates based on EDB-ID
            seen_ids = set()
            unique_exploits = []
            for exploit in exploits:
                edb_id = exploit.get('edb_id')
                if edb_id and edb_id not in seen_ids:
                    seen_ids.add(edb_id)
                    unique_exploits.append(exploit)
            
            logging.info(f"Found {len(unique_exploits)} exploits for {cve_id} on ExploitDB")
            return unique_exploits[:limit]
            
        except Exception as e:
            logging.error(f"Error searching ExploitDB for {cve_id}: {str(e)}")
            return []
    
    def search_recent_exploits(self, days: int = 7, limit: int = 100) -> List[Dict]:
        """
        Search for recently published exploits.
        
        Args:
            days (int): Number of days to look back
            limit (int): Maximum number of results to return
            
        Returns:
            List[Dict]: List of recent exploit data
        """
        exploits = []
        
        try:
            # Calculate date range
            end_date = datetime.utcnow()
            start_date = end_date - timedelta(days=days)
            
            # Search for recent exploits
            recent_exploits = self._get_recent_exploits(start_date, end_date, limit)
            
            # Filter for CVE-related exploits
            for exploit in recent_exploits:
                cve_ids = self._extract_cve_ids(exploit.get('description', '') + ' ' + exploit.get('title', ''))
                if cve_ids:
                    for cve_id in cve_ids:
                        exploit_copy = exploit.copy()
                        exploit_copy['cve_id'] = cve_id
                        exploits.append(exploit_copy)
                else:
                    # Include even if no specific CVE mentioned
                    exploit['cve_id'] = None
                    exploits.append(exploit)
            
            logging.info(f"Found {len(exploits)} recent exploits on ExploitDB")
            return exploits[:limit]
            
        except Exception as e:
            logging.error(f"Error searching recent exploits on ExploitDB: {str(e)}")
            return []
    
    def _search_via_api(self, cve_id: str, limit: int) -> List[Dict]:
        """Search using ExploitDB API."""
        try:
            params = {
                'cve': cve_id,
                'draw': 1,
                'start': 0,
                'length': min(limit, 100)  # API limit
            }
            
            response = self.session.get(self.api_url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            exploits = []
            
            if 'data' in data:
                for item in data['data']:
                    exploit_data = self._parse_api_result(item, cve_id)
                    if exploit_data:
                        exploits.append(exploit_data)
            
            return exploits
            
        except Exception as e:
            logging.warning(f"API search failed for {cve_id}: {str(e)}")
            return []
    
    def _search_via_web(self, cve_id: str, limit: int) -> List[Dict]:
        """Search using web scraping."""
        try:
            params = {
                'action': 'search',
                'description': cve_id,
                'e_author': '',
                'e_platform': '',
                'e_type': '',
                'port': '',
                'orderby': 'date',
                'order': 'desc'
            }
            
            response = self.session.get(self.search_url, params=params, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            exploits = []
            
            # Find the results table
            table = soup.find('table', {'id': 'exploits-table'})
            if not table:
                return []
            
            rows = table.find('tbody').find_all('tr') if table.find('tbody') else []
            
            for row in rows[:limit]:
                exploit_data = self._parse_web_result(row, cve_id)
                if exploit_data:
                    exploits.append(exploit_data)
            
            return exploits
            
        except Exception as e:
            logging.warning(f"Web search failed for {cve_id}: {str(e)}")
            return []
    
    def _get_recent_exploits(self, start_date: datetime, end_date: datetime, limit: int) -> List[Dict]:
        """Get recently published exploits."""
        try:
            # Use the main page or recent exploits endpoint
            response = self.session.get(f"{self.base_url}/", timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            exploits = []
            
            # Look for recent exploits section
            recent_section = soup.find('div', {'class': 'recent-exploits'}) or soup.find('table', {'id': 'exploits-table'})
            
            if recent_section:
                rows = recent_section.find_all('tr')[1:]  # Skip header
                
                for row in rows[:limit]:
                    exploit_data = self._parse_web_result(row)
                    if exploit_data:
                        # Check if within date range
                        pub_date = exploit_data.get('published_date')
                        if pub_date and start_date <= pub_date <= end_date:
                            exploits.append(exploit_data)
            
            return exploits
            
        except Exception as e:
            logging.warning(f"Failed to get recent exploits: {str(e)}")
            return []
    
    def _parse_api_result(self, item: Dict, cve_id: str) -> Optional[Dict]:
        """Parse API result into our format."""
        try:
            # API response structure may vary, adapt as needed
            edb_id = item.get('id')
            title = item.get('description', '')
            author = item.get('author', '')
            platform = item.get('platform', '')
            exploit_type = item.get('type', '')
            date_str = item.get('date', '')
            
            # Parse date
            published_date = None
            if date_str:
                try:
                    published_date = datetime.strptime(date_str, '%Y-%m-%d')
                except:
                    pass
            
            # Build URL
            url = f"{self.base_url}/exploits/{edb_id}" if edb_id else None
            
            # Calculate quality score
            quality_score = self._calculate_quality_score(title, author, platform, exploit_type)
            
            return {
                'cve_id': cve_id,
                'source': 'exploitdb',
                'edb_id': edb_id,
                'title': title,
                'description': title,  # ExploitDB uses title as description
                'url': url,
                'author': author,
                'published_date': published_date,
                'updated_date': published_date,
                'platform': platform,
                'exploit_type': exploit_type,
                'quality_score': quality_score,
                'verified': True,  # ExploitDB exploits are generally verified
                'language': self._detect_language_from_title(title),
                'exploit_files': [f"edb-{edb_id}.txt"] if edb_id else []
            }
            
        except Exception as e:
            logging.error(f"Error parsing API result: {str(e)}")
            return None
    
    def _parse_web_result(self, row, cve_id: str = None) -> Optional[Dict]:
        """Parse web scraping result into our format."""
        try:
            cells = row.find_all('td')
            if len(cells) < 5:
                return None
            
            # Extract data from table cells
            date_cell = cells[0]
            title_cell = cells[1] 
            author_cell = cells[2]
            platform_cell = cells[3]
            type_cell = cells[4]
            
            # Extract date
            date_str = date_cell.get_text(strip=True)
            published_date = None
            try:
                published_date = datetime.strptime(date_str, '%Y-%m-%d')
            except:
                pass
            
            # Extract title and EDB ID
            title_link = title_cell.find('a')
            if not title_link:
                return None
            
            title = title_link.get_text(strip=True)
            href = title_link.get('href', '')
            
            # Extract EDB ID from URL
            edb_id = None
            edb_match = re.search(r'/exploits/(\d+)', href)
            if edb_match:
                edb_id = edb_match.group(1)
            
            # Build full URL
            url = f"{self.base_url}{href}" if href.startswith('/') else href
            
            # Extract other fields
            author = author_cell.get_text(strip=True)
            platform = platform_cell.get_text(strip=True)
            exploit_type = type_cell.get_text(strip=True)
            
            # If no CVE provided, try to extract from title
            if not cve_id:
                cve_ids = self._extract_cve_ids(title)
                cve_id = list(cve_ids)[0] if cve_ids else None
            
            # Calculate quality score
            quality_score = self._calculate_quality_score(title, author, platform, exploit_type)
            
            return {
                'cve_id': cve_id,
                'source': 'exploitdb',
                'edb_id': edb_id,
                'title': title,
                'description': title,
                'url': url,
                'author': author,
                'published_date': published_date,
                'updated_date': published_date,
                'platform': platform,
                'exploit_type': exploit_type,
                'quality_score': quality_score,
                'verified': True,
                'language': self._detect_language_from_title(title),
                'exploit_files': [f"edb-{edb_id}.txt"] if edb_id else []
            }
            
        except Exception as e:
            logging.error(f"Error parsing web result: {str(e)}")
            return None
    
    def _calculate_quality_score(self, title: str, author: str, platform: str, exploit_type: str) -> float:
        """Calculate quality score for ExploitDB exploit."""
        score = 5.0  # Base score for ExploitDB (trusted source)
        
        # Bonus for detailed title
        if len(title) > 50:
            score += 1.0
        
        # Bonus for known author
        if author and author.lower() not in ['unknown', 'anonymous', '']:
            score += 1.0
        
        # Bonus for specific platform
        if platform and platform.lower() not in ['multiple', 'unknown', '']:
            score += 0.5
        
        # Bonus for specific exploit type
        if exploit_type and 'remote' in exploit_type.lower():
            score += 1.0
        elif exploit_type and 'local' in exploit_type.lower():
            score += 0.5
        
        # Bonus for CVE mention in title
        if re.search(r'CVE-\d{4}-\d+', title, re.IGNORECASE):
            score += 1.0
        
        return min(score, 10.0)
    
    def _detect_language_from_title(self, title: str) -> str:
        """Detect programming language from exploit title."""
        title_lower = title.lower()
        
        language_indicators = {
            'python': ['python', '.py', 'python3'],
            'c': [' c ', '.c ', 'gcc'],
            'perl': ['perl', '.pl'],
            'ruby': ['ruby', '.rb'],
            'php': ['php', '.php'],
            'shell': ['shell', 'bash', '.sh'],
            'powershell': ['powershell', '.ps1'],
            'javascript': ['javascript', 'js', 'node'],
            'java': ['java', '.java'],
            'metasploit': ['metasploit', 'msf']
        }
        
        for language, indicators in language_indicators.items():
            if any(indicator in title_lower for indicator in indicators):
                return language.title()
        
        return 'Unknown'
    
    def _extract_cve_ids(self, text: str) -> Set[str]:
        """Extract CVE IDs from text."""
        cve_pattern = r'CVE-\d{4}-\d+'
        matches = re.findall(cve_pattern, text, re.IGNORECASE)
        return set(match.upper() for match in matches)
    
    def get_exploit_details(self, edb_id: str) -> Optional[Dict]:
        """
        Get detailed information about a specific exploit.
        
        Args:
            edb_id (str): ExploitDB ID
            
        Returns:
            Optional[Dict]: Detailed exploit information
        """
        try:
            url = f"{self.base_url}/exploits/{edb_id}"
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract exploit code
            code_section = soup.find('div', {'class': 'exploit-code'}) or soup.find('pre')
            exploit_code = code_section.get_text() if code_section else ''
            
            # Extract metadata
            info_section = soup.find('div', {'class': 'exploit-info'})
            metadata = {}
            
            if info_section:
                for item in info_section.find_all('div', {'class': 'info-item'}):
                    label = item.find('span', {'class': 'label'})
                    value = item.find('span', {'class': 'value'})
                    if label and value:
                        metadata[label.get_text(strip=True)] = value.get_text(strip=True)
            
            return {
                'edb_id': edb_id,
                'url': url,
                'exploit_code': exploit_code,
                'metadata': metadata,
                'code_length': len(exploit_code)
            }
            
        except Exception as e:
            logging.error(f"Error getting exploit details for EDB-{edb_id}: {str(e)}")
            return None
    
    def save_exploits_to_database(self, exploits_data: List[Dict]) -> int:
        """
        Save exploits to the database.
        
        Args:
            exploits_data (List[Dict]): List of exploit data
            
        Returns:
            int: Number of exploits saved
        """
        saved_count = 0
        
        with get_db() as db:
            for exploit_data in exploits_data:
                try:
                    # Check if exploit already exists (by EDB ID or URL)
                    existing = None
                    if exploit_data.get('edb_id'):
                        existing = db.query(Exploit).filter(
                            Exploit.source == 'exploitdb',
                            Exploit.title.contains(f"EDB-{exploit_data['edb_id']}")
                        ).first()
                    
                    if not existing and exploit_data.get('url'):
                        existing = db.query(Exploit).filter(
                            Exploit.url == exploit_data['url']
                        ).first()
                    
                    if existing:
                        # Update existing exploit
                        for key, value in exploit_data.items():
                            if hasattr(existing, key) and key not in ['id', 'created_at']:
                                setattr(existing, key, value)
                        existing.updated_at = datetime.utcnow()
                    else:
                        # Create new exploit
                        exploit = Exploit(**exploit_data)
                        db.add(exploit)
                        saved_count += 1
                    
                    db.commit()
                    
                except Exception as e:
                    logging.error(f"Error saving exploit {exploit_data.get('url', 'unknown')}: {str(e)}")
                    db.rollback()
                    continue
        
        logging.info(f"Saved {saved_count} new exploits from ExploitDB to database")
        return saved_count 